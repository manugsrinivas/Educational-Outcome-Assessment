---
title: "District Data Set"
output: html_notebook
---

```{r}
library(dplyr)
library(readr)
library(tidyr)
library(tidyverse) 
library(caret)    
library(MASS)   
library(car)   
library(pROC)        
library(randomForest) 
library(pdp)         
library(gbm)         
library(e1071)      
```
so basically we are testing if a students score is reliant on its surrounding, and would you be able to predict a district average.


```{r}
# Cleaning code
Data <- read_csv("DistrictData.csv")
# Removing the NA's
Data <- na.omit(Data)
# Removing Year, leaid, District, State Name
Data <- Data[-c(1:4)]
# took our factor to do later
# Making a second version of Data
Data2 <- Data
# renaming the column
Data2 <- Data2 |> rename(
  pos_outcome = outcomegap)
# Making outcomegap a binary variable
# True when pos, False when neg
Data2$pos_outcome <- factor(Data2$pos_outcome > 0)
str(Data2)

```
Experimenting:
West (district) = (Arizona  Colorado  Idaho  New Mexico  Montana  Utah  Nevada Wyoming  Alaska  California  Hawaii  Oregon  Washington)


```{r}
df <- Data 
west <- c("AZ", "CO", "ID", "NM", "MT", "UT", "NV", "WY", "AK", "CA", "HI", "OR", "WA")
# filter
df_west <- df %>% 
  filter(stabbr %in% west)

df_west <- df_west %>%
  mutate(class_outcome = factor(ifelse(outcomegap > 0, "Above", "Below"),
                                levels = c("Below", "Above")))

# Making State a factor
df_west$stabbr <- factor(df_west$stabbr)
str(df_west)
summary(df_west)
```


Seeing what we're working with  Plots:

```{r}

#Bar Plot of Average Outcome Gap by State
# Compute the average outcome gap for each state in the western region
state_summary <- df_west %>%
  group_by(stabbr) %>%
  summarize(avg_outcomegap = mean(outcomegap, na.rm = TRUE),
            n = n()) %>%
  ungroup()

# Create a bar plot showing the average outcome gap per state.
# A horizontal dashed red line at 0 indicates the national average.
ggplot(state_summary, aes(x = reorder(stabbr, avg_outcomegap), y = avg_outcomegap)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(avg_outcomegap, 2)), vjust = ifelse(state_summary$avg_outcomegap >= 0, -0.5, 1.5), size = 3.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Average Outcome Gap by Western State",
       subtitle = "Red dashed line indicates the national average (0 SD)",
       x = "State",
       y = "Average Outcome Gap (SD)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


# Outcome Gap Distribution
ggplot(df, aes(x = outcomegap)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Outcome Gap",
       x = "Outcome Gap",
       y = "Frequency")

# Relationship between the resource variable (fundinggap) and outcome gap
ggplot(df, aes(x = fundinggap, y = outcomegap)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Outcome Gap vs. Funding Gap",
       x = "Funding Gap",
       y = "Outcome Gap")

# Demographic predictor relationship and percentage of students in poverty (pov)
ggplot(df, aes(x = pov, y = outcomegap)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Outcome Gap vs. Poverty Rate",
       x = "Poverty Rate",
       y = "Outcome Gap")

# Histogram of the outcome gap in western districts
ggplot(df_west, aes(x = outcomegap)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Outcome Gap (Western Districts)",
       x = "Outcome Gap (Test Score Difference, SD)",
       y = "Frequency")

# Scatter plot: fundinggap vs. outcomegap
ggplot(df_west, aes(x = fundinggap, y = outcomegap)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Funding Gap vs. Outcome Gap (Western Districts)",
       x = "Funding Gap (Actual - Required Spending Per-Pupil)",
       y = "Outcome Gap")

# Boxplot: Enrollment across performance classes
ggplot(df_west, aes(x = class_outcome, y = enroll)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Enrollment by Performance Category",
       x = "Performance Category",
       y = "Enrollment")


```


Question: How do variations in school resources and demographics affect a school's test score performance compared to the national benchmark?


```{r}

# Full model 
model_full <- glm(class_outcome ~ fundinggap + enroll + pov + iep + ell +
                    amind + asian + black + hisp + multi + pac,
                  data = df_west,
                  family = binomial)
summary(model_full)

# Stepwise model selection (evaluated using AIC)
model_step <- stepAIC(model_full, direction = "both")
summary(model_step)

# the default diagnostic plots 
par(mfrow = c(2, 2))
plot(model_step)
par(mfrow = c(1, 1))

```



Creating ma models:

```{r}
set.seed(123) 
train_index <- createDataPartition(df_west$class_outcome, p = 0.7, list = FALSE)
train_west <- df_west[train_index, ]
test_west  <- df_west[-train_index, ]

# The final model on the training set using the formula from the stepwise selection so we've proven they are necessary predictors
model_final <- glm(formula(model_step), data = train_west, family = binomial)
summary(model_final)

#Test set
# Predicted probabilities and class predictions on the test set.
test_west <- test_west %>%
  mutate(pred_prob = predict(model_final, newdata = test_west, type = "response"),
         pred_class = factor(ifelse(pred_prob > 0.5, "Above", "Below"),
                             levels = c("Below", "Above")))

# Confusion matrix.
conf_mat <- confusionMatrix(test_west$pred_class, test_west$class_outcome)
print(conf_mat)
```

# Classification
```{r}
# omit the "white" category to avoid multicollinearity  ( used it as a base ) - was auto done before
# Logistic regression model
full_formula <- class_outcome ~ fundinggap + enroll + pov + iep + ell +
                            amind + asian + black + hisp + multi + pac



# logistic regression model based off of the training data
model_full <- glm(full_formula, data = train_west, family = binomial)
summary(model_full)

# Stepwise again - just in case 
model_step <- stepAIC(model_full, direction = "both")
summary(model_step)

# Diagnostic plots
par(mfrow = c(2, 2))
plot(model_step)
par(mfrow = c(1, 1))  

# Check for multicollinearity(just in case):
vif_values <- vif(model_step)
print(vif_values)

```

Check How accurate it the model in classifying

```{r}

# Using the stepwise-selected model to predict on the test set.
test_west <- test_west %>%
  mutate(pred_prob = predict(model_step, newdata = test_west, type = "response"),
         pred_class = factor(ifelse(pred_prob > 0.5, "Above", "Below"),
                             levels = c("Below", "Above")))

# Confusion matrix to compare predicted vs. actual classifications.
conf_mat <- confusionMatrix(test_west$pred_class, test_west$class_outcome)
print(conf_mat)

```


```{r}

# Cross-Validation for Threshold Optimization (Logistic Model)
thresholds <- seq(0.3, 0.7, by = 0.05)
set.seed(123)
folds <- createFolds(train_west$class_outcome, k = 10, list = TRUE, returnTrain = FALSE)

cv_accuracy <- sapply(thresholds, function(thresh) {
  fold_acc <- sapply(folds, function(idx) {
    training_fold <- train_west[-idx, ]
    validation_fold <- train_west[idx, ]
    
    mod_fold <- glm(formula(model_step), data = training_fold, family = binomial)
    pred_prob_fold <- predict(mod_fold, newdata = validation_fold, type = "response")
    pred_class_fold <- factor(ifelse(pred_prob_fold > thresh, "Above", "Below"),
                              levels = c("Below", "Above"))
    mean(pred_class_fold == validation_fold$class_outcome)
  })
  mean(fold_acc)
})

cv_results <- data.frame(Threshold = thresholds, CV_Accuracy = cv_accuracy)
print(cv_results)

best_threshold <- cv_results$Threshold[which.max(cv_results$CV_Accuracy)]
cat("Optimal Threshold based on CV Accuracy:", best_threshold, "\n")

# Evaluate logistic model on test set using the optimal threshold.
test_west <- test_west %>%
  mutate(pred_prob = predict(model_step, newdata = test_west, type = "response"),
         pred_class = factor(ifelse(pred_prob > best_threshold, "Above", "Below"),
                             levels = c("Below", "Above")))
conf_mat_logit <- confusionMatrix(test_west$pred_class, test_west$class_outcome)
print(conf_mat_logit)

roc_obj_logit <- roc(test_west$class_outcome, test_west$pred_prob, levels = c("Below", "Above"))
auc_logit <- auc(roc_obj_logit)
cat("Logistic Model AUC:", auc_logit, "\n")
plot(roc_obj_logit, main = "ROC Curve: Logistic Regression", col = "blue")

# Random Forest
# Train a random forest model using the same formula (excluding state fixed effects cause idk how to do all that)
set.seed(123)
rf_model <- randomForest(full_formula, data = train_west, ntree = 500, importance = TRUE)
print(rf_model)

# Random forest predictions (same best threshold from logistic model for comparability)
rf_pred_prob <- predict(rf_model, newdata = test_west, type = "prob")[, "Above"]
rf_pred_class <- factor(ifelse(rf_pred_prob > best_threshold, "Above", "Below"),
                        levels = c("Below", "Above"))

conf_mat_rf <- confusionMatrix(rf_pred_class, test_west$class_outcome)
print(conf_mat_rf)

roc_obj_rf <- roc(test_west$class_outcome, rf_pred_prob, levels = c("Below", "Above"))
auc_rf <- auc(roc_obj_rf)
cat("Random Forest Model AUC:", auc_rf, "\n")
plot(roc_obj_rf, main = "ROC Curve: Random Forest", col = "darkgreen")

# Plot variable importance for the random forest model
varImpPlot(rf_model, main = "Random Forest Variable Importance")


#Partial Dependence Plots (PDP) for Key Predictors in Random Forest

# PDP for fundinggap – how it affects the probability of being "Above"
pdp_fundinggap <- partial(rf_model, pred.var = "fundinggap", which.class = 2)
plotPartial(pdp_fundinggap, main = "Partial Dependence: Funding Gap", xlab = "Funding Gap")

# PDP for pov (poverty rate)
pdp_pov <- partial(rf_model, pred.var = "pov", which.class = 2)
plotPartial(pdp_pov, main = "Partial Dependence: Poverty Rate", xlab = "Poverty Rate")

```
There might be over fitting in random forest?? will check that

# required models

```{r}
# Note: LDA and QDA do not work well with factor predictors.
# throw out the states and include just the numeric variables
basic_formula <- class_outcome ~ fundinggap + enroll + pov + iep + ell +
                           amind + asian + black + hisp + multi + pac


# training on 10 fold 
train_control <- trainControl(method = "cv", number = 10,
                              classProbs = TRUE, summaryFunction = defaultSummary)

# the models
set.seed(123) # just in case yk
# Logistic Regression
model_glm <- train(basic_formula,
                   data = train_west,
                   method = "glm",
                   family = binomial,
                   trControl = train_control,
                   metric = "Accuracy")

#Linear Discriminant Analysis (LDA)
set.seed(123)
model_lda <- train(basic_formula,
                   data = train_west,
                   method = "lda",
                   trControl = train_control,
                   metric = "Accuracy")

# Quadratic Discriminant Analysis (QDA)
set.seed(123)
model_qda <- train(basic_formula,
                   data = train_west,
                   method = "qda",
                   trControl = train_control,
                   metric = "Accuracy")

# Classification Trees (rpart)
set.seed(123)
model_tree <- train(basic_formula,
                    data = train_west,
                    method = "rpart",
                    trControl = train_control,
                    metric = "Accuracy")

# Random Forest
set.seed(123)
model_rf <- train(basic_formula,
                  data = train_west,
                  method = "rf",
                  trControl = train_control,
                  metric = "Accuracy")

# Boosting (GBM)
set.seed(123)
model_gbm <- train(basic_formula,
                   data = train_west,
                   method = "gbm",
                   trControl = train_control,
                   metric = "Accuracy",
                   verbose = FALSE)

# Support Vector Machine (Radial Kernel)
set.seed(123)
model_svm <- train(basic_formula,
                   data = train_west,
                   method = "svmRadial",
                   trControl = train_control,
                   metric = "Accuracy")

# k-Nearest Neighbors (kNN)
set.seed(123)
model_knn <- train(basic_formula,
                   data = train_west,
                   method = "knn",
                   trControl = train_control,
                   metric = "Accuracy")


# Comparing
results <- resamples(list(GLM = model_glm,
                          LDA = model_lda,
                          QDA = model_qda,
                          Tree = model_tree,
                          RF = model_rf,
                          GBM = model_gbm,
                          SVM = model_svm,
                          kNN = model_knn))
summary(results) 
bwplot(results, metric = "Accuracy")

```




```{r}
# Testing on test data
model_list <- list(
  GLM  = model_glm,
  LDA  = model_lda,
  QDA  = model_qda,
  Tree = model_tree,
  RF   = model_rf,
  GBM  = model_gbm,
  SVM  = model_svm,
  kNN  = model_knn
)

for(model_name in names(model_list)) {
  cat("---------", model_name, "---------\n")
  
  current_model <- model_list[[model_name]]
  
  # Predict probability for class "Above". 
  # (Assumes that the caret model supports type = "prob"; if not, you may need to adjust.)
  pred_prob <- predict(current_model, newdata = test_west, type = "prob")[, "Above"]
  
  # Use a threshold of 0.5 to classify observations.
  pred_class <- factor(ifelse(pred_prob > 0.5, "Above", "Below"),
                       levels = c("Below", "Above"))
  
  # Compute the confusion matrix.
  cm <- confusionMatrix(pred_class, test_west$class_outcome)
  print(cm)
  
  # Compute the ROC curve and AUC.
  roc_obj <- roc(test_west$class_outcome, pred_prob, levels = c("Below", "Above"))
  auc_value <- auc(roc_obj)
  cat(model_name, "AUC:", auc_value, "\n")
  
  # Plot the ROC curve.
  plot(roc_obj, main = paste("ROC Curve:", model_name), col = "blue")
  
  cat("\n\n")
}

```


```{r}
# Example: Evaluate Overfitting in the Random Forest Model 

# Training performance (out-of-bag error)
rf_model <- randomForest(basic_formula, data = train_west, ntree = 500, importance = TRUE)
print(rf_model)  # Look at the OOB error reported in the model summary.

# Test Set Performance
rf_pred_prob <- predict(rf_model, newdata = test_west, type = "prob")[, "Above"]
rf_pred_class <- factor(ifelse(rf_pred_prob > 0.5, "Above", "Below"), levels = c("Below", "Above"))
conf_mat_rf <- confusionMatrix(rf_pred_class, test_west$class_outcome)
print(conf_mat_rf)


# Learning Curves: Varying training data size to see convergence of error rates
library(ggplot2)
train_sizes <- seq(0.1, 1, by = 0.1)
train_errors <- c()
cv_errors <- c()

set.seed(123)
for (frac in train_sizes) {
  # fraction of the training data
  sub_train <- train_west %>% sample_frac(frac)
  
  # model
  mod_sub <- randomForest(basic_formula, data = sub_train, ntree = 500)
  
  # Training error:  OOB error from the random forest
  train_errors <- c(train_errors, mod_sub$err.rate[500, "OOB"])
  
  # Cross-validated error using, say, 5-fold CV on this subtraining set
  cv_result <- train(basic_formula,
                     data = sub_train,
                     method = "rf",
                     trControl = trainControl(method = "cv", number = 5),
                     ntree = 500,
                     metric = "Accuracy")
  cv_error <- 1 - max(cv_result$results$Accuracy)
  cv_errors <- c(cv_errors, cv_error)
}

# Combine results for plotting
learning_curve <- data.frame(Train_Size = train_sizes,
                             OOB_Error = train_errors,
                             CV_Error = cv_errors)

# Plot learning curves
ggplot(learning_curve, aes(x = Train_Size)) +
  geom_line(aes(y = OOB_Error, color = "OOB Error")) +
  geom_line(aes(y = CV_Error, color = "CV Error")) +
  labs(title = "Learning Curves for Random Forest Model",
       x = "Fraction of Training Data",
       y = "Error Rate") +
  scale_color_manual(name = "Error Type", values = c("OOB Error" = "blue", "CV Error" = "red")) +
  theme_minimal()

```

OOB Error:
The reported OOB error is 11.8%. The model misclassifies about 11.8% of the training instances.

For the "Below" class:
Out of the observations, 6194 were correctly classified as “Below” with a misclassification (class.error) of about 7.18%.

For the "Above" class:
2764 were correctly predicted as “Above”, but 720 were misclassified, resulting in a higher error rate of around 20.67%.
This difference indicates that the model has a bit more difficulty accurately classifying the “Above” class compared to “Below.”

Test Set Performance (Using Caret's Confusion Matrix)
Overall Accuracy:
The test set accuracy is 87.61% with a 95% confidence interval ranging from 86.6% to 88.6%. This is substantially higher than the No Information Rate (65.71%), indicating that your model performs much better than a naive classifier that always picks the most common class.

Class-Specific Metrics:

Sensitivity (93.70%) for the "Below" class:
This high sensitivity means that the model correctly identifies 93.7% of the districts that truly belong to the “Below” category.

Specificity (75.94%) for the "Above" class:
This indicates that 75.9% of the “Above” districts are correctly classified. The lower specificity suggests that some “Above” cases are being misclassified as “Below.”

Generalization:
The OOB error (~11.8%) aligns closely with the test set error (approximately 12.4% when considering 100% minus 87.61% accuracy), implying that overfitting is not a major concern in this model. The model seems to generalize well from the training data to the unseen test data.

Class Imbalance or Difficulty:
The higher error rate (lower specificity) for the “Above” category might indicate that the features are more predictive of a district being “Below” the national average, or that the “Above” class has more inherent variability. You might experiment with alternative classification thresholds or even consider cost-sensitive learning if distinguishing the “Above” cases becomes critical.

The random forest model demonstrates strong performance with high accuracy and good generalization. Even though the “Above” class presents a slightly higher misclassification rate, overall the model's performance metrics—especially the high sensitivity for the “Below” class and a balanced accuracy of around 85%—suggest that your approach is both robust and reliable. This makes your model a promising tool for understanding how variations in school resources and demographics relate to performance compared to the national benchmark.



